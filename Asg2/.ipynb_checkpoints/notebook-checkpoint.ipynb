{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "> Remember to add an explanation of what you do using markdown, and to comment your code. Please *be brief*.\n",
    ">\n",
    "> If you re-use a substantial portion of code you find online, e.g on Stackoverflow, you need to add a link to it and make the borrowing explicit. The same applies of you take it and modify it, even substantially. There is nothing bad in doing that, providing you are acknowledging it and make it clear you know what you're doing.\n",
    ">\n",
    "> Make sure your notebooks have been run when you sumit, as I won't run them myself. Submit both the `.ipynb` file along with an `.html` export of the same. Submit all necessary auxilliary files as well. Please compress your submission into a `.zip` archive. Only `.zip` files can be submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading policy\n",
    "> As follows:\n",
    ">\n",
    "> * 70 points for correctly completing the assignment.\n",
    ">\n",
    "> * 20 points for appropriately writing and organizing your code in terms of structure, readibility (also by humans), comments and minimal documentation. It is important to be concise but also to explain what you did and why, when not obvious.\n",
    "> \n",
    "> * 10 points for doing something extra, e.g., if you go beyond expectations (overall or on something specific). Some ideas for extras might be mentioned in the exercises, or you can come up with your own. You don't need to do them all to get the bonus. The sum of points is 90, doing (some of) the extras can bring you to 100, so the extras are not necessary to get an A.\n",
    "> \n",
    "\n",
    "**The AUC code of conduct applies to this assignment: please only submit your own work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this assignment, you will build and compare vector models for measuring **semantic similarity**.\n",
    "\n",
    "First, you are going to use different count-based methods to create these models. Secondly you are going to created dense, lower-dimensionality models from them. Thirdly, you are going to use prediction-based models as well.\n",
    "\n",
    "Eventually, you are asked to assess the performance of these models against a human gold standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus preparation (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (10 points)\n",
    "\n",
    "Create one distributional space by **counting and filtering** the surface co-occurrences in a symmetric ±5 word collocations span from the following corpus:\n",
    "\n",
    "* A lemmatized version of the Reuters corpus (the choice of the lemmatizer is up to you). For this step, you might need a PoS-tagger: you are welcome to choose one yourself. In case you can't do PoS tagging on your own, you can use the following command to load the provided corpus in `data/reuters.pos` (uploaded as a `.zip` file, so first unzip it):\n",
    "\n",
    "```python\n",
    "with open(\"data/reuters.pos\", \"rb\") as corpus_file:\n",
    "    reuter_PoSTagged = pickle.load(corpus_file)\n",
    "```\n",
    "\n",
    "Remember to make motivated choices for the different strategies in building word vectors as described in class. Be explicit about:\n",
    "\n",
    "1. what lemmas you want to describe (i.e., what will be your target vectors?);\n",
    "2. how you want to describe them (i.e., what will be your contexts?);\n",
    "3. what filtering strategy you are going to choose (i.e., what do you exclude?).\n",
    "\n",
    "\n",
    "1. The target vectors I selected are nouns?????\n",
    "2. The words will be described by the context of surface co-occuring words with a ±5 word symmetric span of  around the target word\n",
    "3. Filtering out words that are: Non-alphabetic, Stopwords, one-character words\n",
    "\n",
    "Sources: the class notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import reuters\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from itertools import chain\n",
    "from nltk.metrics import BigramAssocMeasures as bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r6/mc8wx4512yq0kcrhmk5qy9rr0000gn/T/ipykernel_8548/594377524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"NOUN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mspan_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mspansize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# left side indices (range, then list so we can extend)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mspan_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspansize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# extend by right side indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspan_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# MY CODE Q1 \n",
    "\n",
    "#List of sentences\n",
    "#text = reuters.sents() \n",
    "text = [['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-',\n",
    "         'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.',\n",
    "         'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia',\n",
    "         \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far',\n",
    "         '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said',\n",
    "         '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U',\n",
    "                '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment',\n",
    "                'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports',\n",
    "                'of', 'their', 'products', '.']]\n",
    "\n",
    "#Pos-tagging words\n",
    "postext = [nltk.pos_tag(word, tagset = \"universal\") for word in text] \n",
    "#print(postext)\n",
    "\n",
    "#Filtering \n",
    "# I remove stop words and alphanumeric words (keeps purely alphabetic words)\n",
    "# and words that are only one character as the text has some random one letter tokens\n",
    "# I store all extracted (not-removed) words in a list\n",
    "wordslist = []\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for sentence in postext:\n",
    "    #item is: \"word-postag\"\n",
    "    sublist = []\n",
    "    for item in sentence:\n",
    "        # item[0] = word & item[1] = pos-tag\n",
    "        if ((len(item[0]) > 1 and not item[0] in stopwords) and (item[0].isalpha())):\n",
    "            sublist.append((item[0].lower(), item[1]))\n",
    "    wordslist.append(sublist)\n",
    "\n",
    "        \n",
    "# Lemmatizing text\n",
    "# I store the lematized text in a list\n",
    "lemmatized = []\n",
    "mapping = {\"VERB\" : wn.VERB, \"NOUN\" : wn.NOUN, \"ADJ\" : wn.ADJ, \"ADV\" : wn.ADV}\n",
    "\n",
    "for sentence in wordslist:\n",
    "    lemmatized_sentence = []\n",
    "    for w, p in sentence:\n",
    "        if p in [\".\", \"X\"]: \n",
    "            continue\n",
    "        elif p in mapping.keys():\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = mapping[p])\n",
    "        else:\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "        lemmatized_sentence.append(\"-\".join([lemma, p]))\n",
    "    lemmatized.append(lemmatized_sentence)\n",
    "    \n",
    "    \n",
    "#since reuters lemma were filtered from the start, I will continue to use this here.\n",
    "# Counting surface co-occurrences in a symmetric ±5 word collocations span\n",
    "# this count is found by counting how many times a contextual word occurs\n",
    "# in a collocational span surrounding the target word\n",
    "\n",
    "spansize = 5\n",
    "\n",
    "cooccs_surface = Counter()\n",
    "for sentence in lemmatized:\n",
    "    for i,w in enumerate(sentence):\n",
    "        if w.split(\"-\")[-1] == \"NOUN\":\n",
    "            span_range = list(range(max(i- spansize, 0), i)) # left side indices (range, then list so we can extend)\n",
    "            span_range.extend(range(i+1, min(i + spansize + 1, len(sentence)))) # extend by right side indices\n",
    "            for cw in [sentence[idx] for idx in span_range]:\n",
    "                cooccs_surface[(w, cw)] += 1\n",
    "#Example                \n",
    "print(cooccs_surface.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector representations (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (20 points)\n",
    "\n",
    "Weight the counts in the space you created for the previous question by using the following association measures on both spaces:\n",
    "\n",
    "1. One **measure of your choice** among those available in the [nltk.BigramAssocMeasures](http://www.nltk.org/howto/metrics.html#association-measures) module.\n",
    "2. The **Positive Local Mutual Information** measure (as shown in class/lab).\n",
    "\n",
    "**Possible extra**\n",
    "\n",
    "3. Also use the **smoothed ppmi measure** proposed by [Levy et al. (2015)](http://www.aclweb.org/anthology/Q15-1016). Recall that the authors proposed to smooth the ppmi by raising the context counts to the power of $\\alpha$ (where $\\alpha= 0.75$ is reported to work well). That is, if $V_c$ is the vocabulary of all the contexts in a given space and $f(c)$ is the context frequency, they proposed the following association measure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$PPMI_\\alpha (w,c) = max \\left(0, \\ log_2 \\left(\\frac{p(w,c)}{p(w) \\cdot p_\\alpha(c)}\\right)  \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$where: \\ \\ p_\\alpha(c) = \\frac{f(c)^\\alpha}{\\sum_{c' \\in V_c} f(c')^\\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "Sources:\n",
    "\n",
    "Class Notebooks\n",
    "\n",
    "[smoothed ppmi](https://web.stanford.edu/~jurafsky/slp3/slides/vector1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common frequencies: []\n",
      ">> 0 context lemmas\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# MY CODE Q2 \n",
    "# Filtering out lowest frequencies as we have too little empirical evidence on them\n",
    "# and to speed up function\n",
    "\n",
    "# all words\n",
    "lemmas_frequencies = Counter(chain(*lemmatized))\n",
    "\n",
    "#print(list(reversed(lemmas_frequencies.most_common()))[:20])  \n",
    "print(\"most common frequencies:\",lemmas_frequencies.most_common()[:20])\n",
    "\n",
    "filtered_lemmas_frequencies = Counter()\n",
    "selected_noun_frequencies = Counter()\n",
    "\n",
    "# why is filtered ^empttyyyy\n",
    "\n",
    "for k,v in lemmas_frequencies.items():\n",
    "    pos = k.split(\"-\")[-1]\n",
    "    if v >= 10 and \\\n",
    "    pos in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\\\n",
    "    and k.split(\"-\")[0] not in [\"be\", \"have\", \"do\", \"would\", \"will\", \"could\"]:\n",
    "        filtered_lemmas_frequencies[k] = v\n",
    "        if pos == \"NOUN\":\n",
    "            selected_noun_frequencies[k] = v\n",
    "\n",
    "print(\">>\", len(filtered_lemmas_frequencies), \"context lemmas\")\n",
    "print(filtered_lemmas_frequencies.most_common(20))\n",
    "\n",
    "\n",
    "filtered_cooccs_surface = Counter()\n",
    "\n",
    "for k,v in cooccs_surface.items():\n",
    "    if k[0] in selected_noun_frequencies and k[1] in filtered_lemmas_frequencies:\n",
    "        filtered_cooccs_surface[k] = v\n",
    "\n",
    "print(filtered_cooccs_surface.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk's BigramAssocMeasure's dice:  []\n"
     ]
    }
   ],
   "source": [
    "# Measure of my choice: \n",
    "\n",
    "bam_dice_surface = Counter()\n",
    "\n",
    "for i, x in filtered_cooccs_surface.items(): #creating counter object with weights for bam.pmi\n",
    "\n",
    "    bam_dice_surface[i] = bam.dice(x, (lemmas_frequencies[i[0]], lemmas_frequencies[i[1]]), N)\n",
    "\n",
    "print(\"nltk's BigramAssocMeasure's dice: \", bampmi_surface.most_common(10)) #measure of my choice\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Positive Local Mutual Information\n",
    "\n",
    "# All functions receive as arguments:\n",
    "#\n",
    "#   o_11>> the joint frequency of w1 and w2 >> f(w1, w2)\n",
    "#   r_1 >> w1 marginals >> f(w1, *)\n",
    "#   c_1 >> w2 marginals >> f(*, w2)\n",
    "#   n >> total number of possible coccorrencies >> f(*, *)\n",
    "\n",
    "# These arguments can be arranged in a contingency table of the form:\n",
    "#\n",
    "#          w2    ~w2\n",
    "#        ------ ------\n",
    "#    w1 | o_11 | o_12 | = r_1\n",
    "#        ------ ------\n",
    "#   ~w1 | o_21 | o_22 | = r_2\n",
    "#        ------ ------\n",
    "#        = c_1  = c_2   = n\n",
    "\n",
    "\n",
    "def plmi(o_11, r_1, c_1, n):\n",
    "    \"\"\"\n",
    "    Positive Local Mutual Information, useful for leveraging the \n",
    "    low-frequency bias of the PPMI\n",
    "    \"\"\"\n",
    "    res = o_11 * ppmi(o_11, r_1, c_1, n)\n",
    "    return res\n",
    "\n",
    "\n",
    "plmis_surface = Counter()\n",
    "\n",
    "N = sum(cooccs_surface.values())  # note that this is NOT the reduced dictionary\n",
    "\n",
    "for k,v in filtered_cooccs_surface.items():\n",
    "    plmis_surface[k] = plmi(v, lemmas_frequencies[k[0]], lemmas_frequencies[k[1]], N)\n",
    "    \n",
    "print(\"positive local mutual information:\",plmis_surface.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smooth ppmi:  []\n"
     ]
    }
   ],
   "source": [
    "# Smoothed ppmi\n",
    "\n",
    "def smooth_ppmi(o_11, r_1, c_1, n):\n",
    "    observed = o_11\n",
    "    probofc = c_1/n\n",
    "    alph = 0.75\n",
    "    palphac = (probofc**alph)/(probofc**alph+(1-probofc)**alph)\n",
    "    \n",
    "    expected = (r_1/n) * palphac\n",
    "    res = log(observed/expected,2)\n",
    "    return max(0, res)\n",
    "\n",
    "smoothppmi_surface = Counter()\n",
    "\n",
    "for i, x in filtered_cooccs_surface.items(): #creating counter object with weights for plmi\n",
    "\n",
    "    smoothppmi_surface[i] = smooth_ppmi(x, reuters_lemmas_frequencies[i[0]], reuters_lemmas_frequencies[i[1]], N)\n",
    "    \n",
    "print(\"smoothed ppmi: \", smoothppmi_surface.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (20 points)\n",
    "\n",
    "Up to this point, you should have created 2 different distributional spaces (3 if you did the extra).\n",
    "\n",
    "Use **Singular Value Decomposition** to reduce their dimensionality retaining only the first 100 dimensions. For this question, you can either re-use the SVD code from the lab, or import the SVD functions from external libraries such as [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) or [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html).\n",
    "\n",
    "**Possible extra**\n",
    "\n",
    "Find the 'optimal' number of dimensions to retain using the approach shown in the lab. Use a model with this dimensionality instead of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MY CODE Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Train a Word2Vec model the same corpus, for example using [gensim](https://radimrehurek.com/gensim). Make sure to motivate the choice of your hyperparameters.\n",
    "\n",
    "**Possible extra**\n",
    "\n",
    "*Fine-tuning* is the process of starting from a pre-trained embedding model and training it some more using new data. Try to use a pre-trained model from gensim and to fine-tune it on the Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY CODE Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on semantic similarity (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 5 (20 points)\n",
    "\n",
    "Evaluate the performance of your models on a **semantic similarity task**. Using `SimLex-999` as gold standard. Evaluate all of your models on the dataset in `data/SimLex-999.txt`, and determine the best performing model. Note: There should be 5 to 8 model evaluations in total. 5 if you did not do any extra (2 from 4.1 + 2 from 4.2 + 1 from 4.3), and 8 if you did them all (3 from 4.1 + 3 from 4.2 + 2 from 4.3).\n",
    "\n",
    "1. Your evaluation should follow the approach shown in lab 4 (Section 1.6: \"Evaluating your Model\"), using a **correlation measure** on model predictions and the (human) gold standard. \n",
    "2. Remember to **visualize** your results (e.g., as bar plots).\n",
    "3. Take note (and report) the overlap between your models and the SimLex-999 dataset, i.e., how many pairs are shared by your model and the evaluation dataset.\n",
    "4. Make sure to discuss your results and provide your reasoning on them.\n",
    "\n",
    "### Remarks\n",
    "\n",
    "- The 'SimLex-999' dataset is described in `data/SimLex-999.README.txt`, and [the author's github page](https://fh295.github.io/simlex.html). Hint: the relevant judgements are those in the `SimLex999` column.\n",
    "- To directly compare the models against the gold standard, you will have to find the *overlap* between them, i.e. the pairs that occur in your model *and* the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word1', 'word2', 'POS', 'SimLex999', 'conc(w1)', 'conc(w2)', 'concQ', 'Assoc(USF)', 'SimAssoc333', 'SD(SimLex)']\n",
      "['old', 'new', 'A', '1.58', '2.72', '2.81', '2', '7.25', '1', '0.41']\n",
      "['smart', 'intelligent', 'A', '9.2', '1.75', '2.46', '1', '7.11', '1', '0.67']\n",
      "['hard', 'difficult', 'A', '8.77', '3.76', '2.21', '2', '5.94', '1', '1.19']\n",
      "['happy', 'cheerful', 'A', '9.55', '2.56', '2.34', '1', '5.85', '1', '2.18']\n",
      "['hard', 'easy', 'A', '0.95', '3.76', '2.07', '2', '5.82', '1', '0.93']\n",
      "['fast', 'rapid', 'A', '8.75', '3.32', '3.07', '2', '5.66', '1', '1.68']\n",
      "['happy', 'glad', 'A', '9.17', '2.56', '2.36', '1', '5.49', '1', '1.59']\n",
      "['short', 'long', 'A', '1.23', '3.61', '3.18', '2', '5.36', '1', '1.58']\n",
      "['stupid', 'dumb', 'A', '9.58', '1.75', '2.36', '1', '5.26', '1', '1.48']\n",
      "['weird', 'strange', 'A', '8.93', '1.59', '1.86', '1', '4.26', '1', '1.3']\n",
      "['wide', 'narrow', 'A', '1.03', '3.06', '3.04', '2', '4.06', '1', '0.58']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/SimLex-999.txt\") as f:\n",
    "    for n, line in enumerate(f.read().split(\"\\n\")):\n",
    "        items = line.split(\"\\t\")\n",
    "        print(items)\n",
    "        if n>10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MY CODE Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
